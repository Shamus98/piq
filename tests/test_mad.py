# -*- coding: utf-8 -*-
"""MADUpdate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10W6-JLI26FsVsrpolWGG9Gb7YjDoS3hy
"""

import numpy as np
import torch
from torchvision import transforms
import torch.nn as nn
import torch.nn.functional as F
import math
import functools



def abs(x):
    return torch.sqrt(x[:,:,:,:,0]**2+x[:,:,:,:,1]**2+1e-12)

def real(x):
    return x[:,:,:,:,0]

def imag(x):
    return x[:,:,:,:,1]

def roll_n(X, axis, n):
    f_idx = tuple(slice(None, None, None) if i != axis else slice(0, n, None) for i in range(X.dim()))
    b_idx = tuple(slice(None, None, None) if i != axis else slice(n, None, None) for i in range(X.dim()))
    front = X[f_idx]
    back = X[b_idx]
    return torch.cat([back, front], axis)

def batch_fftshift2d(x):
    real, imag = torch.unbind(x, -1)
    for dim in range(1, len(real.size())):
        n_shift = real.size(dim)//2
        if real.size(dim) % 2 != 0:
            n_shift += 1  # for odd-sized images
        real = roll_n(real, axis=dim, n=n_shift)
        imag = roll_n(imag, axis=dim, n=n_shift)
    return torch.stack((real, imag), -1)  # last dim=2 (real&imag)

def batch_ifftshift2d(x):
    real, imag = torch.unbind(x, -1)
    for dim in range(len(real.size()) - 1, 0, -1):
        real = roll_n(real, axis=dim, n=real.size(dim)//2)
        imag = roll_n(imag, axis=dim, n=imag.size(dim)//2)
    return torch.stack((real, imag), -1)  # last dim=2 (real&imag)

def preprocess_lab(lab):
		L_chan, a_chan, b_chan =torch.unbind(lab,dim=2)
		# L_chan: black and white with input range [0, 100]
		# a_chan/b_chan: color channels with input range ~[-110, 110], not exact
		# [0, 100] => [-1, 1],  ~[-110, 110] => [-1, 1]
		return [L_chan / 50.0 - 1.0, a_chan / 110.0, b_chan / 110.0]

def deprocess_lab(L_chan, a_chan, b_chan):
		#TODO This is axis=3 instead of axis=2 when deprocessing batch of images 
			   # ( we process individual images but deprocess batches)
		#return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)
		return torch.stack([(L_chan + 1) / 2.0 * 100.0, a_chan * 110.0, b_chan * 110.0], dim=2)

def rgb_to_lab(srgb):
    srgb = srgb/255
    srgb_pixels = torch.reshape(srgb, [-1, 3])
    linear_mask = (srgb_pixels <= 0.04045).type(torch.FloatTensor).to(device)
    exponential_mask = (srgb_pixels > 0.04045).type(torch.FloatTensor).to(device)
    rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask
	
    rgb_to_xyz = torch.tensor([
				#    X        Y          Z
				[0.412453, 0.212671, 0.019334], # R
				[0.357580, 0.715160, 0.119193], # G
				[0.180423, 0.072169, 0.950227], # B
			]).type(torch.FloatTensor).to(device)
	
    xyz_pixels = torch.mm(rgb_pixels, rgb_to_xyz)
	

    # XYZ to Lab
    xyz_normalized_pixels = torch.mul(xyz_pixels, torch.tensor([1/0.950456, 1.0, 1/1.088754]).type(torch.FloatTensor).to(device))

    epsilon = 6.0/29.0
    linear_mask = (xyz_normalized_pixels <= (epsilon**3)).type(torch.FloatTensor).to(device)
    exponential_mask = (xyz_normalized_pixels > (epsilon**3)).type(torch.FloatTensor).to(device)
    fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon**2) + 4.0/29.0) * linear_mask + ((xyz_normalized_pixels+0.000001) ** (1.0/3.0)) * exponential_mask
    # convert to lab
    fxfyfz_to_lab = torch.tensor([
        #  l       a       b
        [  0.0,  500.0,    0.0], # fx
        [116.0, -500.0,  200.0], # fy
        [  0.0,    0.0, -200.0], # fz
    ]).type(torch.FloatTensor).to(device)
    lab_pixels = torch.mm(fxfyfz_pixels, fxfyfz_to_lab) + torch.tensor([-16.0, 0.0, 0.0]).type(torch.FloatTensor).to(device)
    #return tf.reshape(lab_pixels, tf.shape(srgb))
    return torch.reshape(lab_pixels, srgb.shape)

def lab_to_rgb(lab):
		lab_pixels = torch.reshape(lab, [-1, 3])
		# convert to fxfyfz
		lab_to_fxfyfz = torch.tensor([
			#   fx      fy        fz
			[1/116.0, 1/116.0,  1/116.0], # l
			[1/500.0,     0.0,      0.0], # a
			[    0.0,     0.0, -1/200.0], # b
		]).type(torch.FloatTensor).to(device)
		fxfyfz_pixels = torch.mm(lab_pixels + torch.tensor([16.0, 0.0, 0.0]).type(torch.FloatTensor).to(device), lab_to_fxfyfz)

		# convert to xyz
		epsilon = 6.0/29.0
		linear_mask = (fxfyfz_pixels <= epsilon).type(torch.FloatTensor).to(device)
		exponential_mask = (fxfyfz_pixels > epsilon).type(torch.FloatTensor).to(device)


		xyz_pixels = (3 * epsilon**2 * (fxfyfz_pixels - 4/29.0)) * linear_mask + ((fxfyfz_pixels+0.000001) ** 3) * exponential_mask

		# denormalize for D65 white point
		xyz_pixels = torch.mul(xyz_pixels, torch.tensor([0.950456, 1.0, 1.088754]).type(torch.FloatTensor).to(device))


		xyz_to_rgb = torch.tensor([
			#     r           g          b
			[ 3.2404542, -0.9692660,  0.0556434], # x
			[-1.5371385,  1.8760108, -0.2040259], # y
			[-0.4985314,  0.0415560,  1.0572252], # z
		]).type(torch.FloatTensor).to(device)

		rgb_pixels =  torch.mm(xyz_pixels, xyz_to_rgb)
		# avoid a slightly negative number messing up the conversion
		#clip
		rgb_pixels[rgb_pixels > 1] = 1
		rgb_pixels[rgb_pixels < 0] = 0

		linear_mask = (rgb_pixels <= 0.0031308).type(torch.FloatTensor).to(device)
		exponential_mask = (rgb_pixels > 0.0031308).type(torch.FloatTensor).to(device)
		srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + (((rgb_pixels+0.000001) ** (1/2.4) * 1.055) - 0.055) * exponential_mask
	
		return torch.reshape(srgb_pixels, lab.shape)

def spatial_normalize(x):
    min_v = torch.min(x.view(x.shape[0],1,-1),dim=2)[0]
    range_v = torch.max(x.view(x.shape[0],1,-1),dim=2)[0] - min_v
    return (x - min_v.unsqueeze(2).unsqueeze(3)) / (range_v.unsqueeze(2).unsqueeze(3)+1e-12)

def fspecial_gauss(size, sigma, channels):
    # Function to mimic the 'fspecial' gaussian MATLAB function
    x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]
    g = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))
    g = torch.from_numpy(g/g.sum()).float().unsqueeze(0).unsqueeze(0)
    return g.repeat(channels,1,1,1)

def downsample(img1, img2, maxSize = 256):
    _,channels,H,W = img1.shape
    f = int(max(1,np.round(min(H,W)/maxSize)))
    if f>1:
        aveKernel = (torch.ones(channels,1,f,f)/f**2).to(img1.device)
        img1 = F.conv2d(img1, aveKernel, stride=f, padding = 0, groups = channels)
        img2 = F.conv2d(img2, aveKernel, stride=f, padding = 0, groups = channels)
    return img1, img2

def extract_patches_2d(img, patch_shape=[64, 64], step=[27,27], batch_first=True, keep_last_patch=False):
    patch_H, patch_W = patch_shape[0], patch_shape[1]
    if(img.size(2)<patch_H):
        num_padded_H_Top = (patch_H - img.size(2))//2
        num_padded_H_Bottom = patch_H - img.size(2) - num_padded_H_Top
        padding_H = nn.ConstantPad2d((0,0,num_padded_H_Top,num_padded_H_Bottom),0)
        img = padding_H(img)
    if(img.size(3)<patch_W):
        num_padded_W_Left = (patch_W - img.size(3))//2
        num_padded_W_Right = patch_W - img.size(3) - num_padded_W_Left
        padding_W = nn.ConstantPad2d((num_padded_W_Left,num_padded_W_Right,0,0),0)
        img = padding_W(img)
    step_int = [0,0]
    step_int[0] = int(patch_H*step[0]) if(isinstance(step[0], float)) else step[0]
    step_int[1] = int(patch_W*step[1]) if(isinstance(step[1], float)) else step[1]
    patches_fold_H = img.unfold(2, patch_H, step_int[0])
    if((img.size(2) - patch_H) % step_int[0] != 0) and keep_last_patch:
        patches_fold_H = torch.cat((patches_fold_H,img[:,:,-patch_H:,].permute(0,1,3,2).unsqueeze(2)),dim=2)
    patches_fold_HW = patches_fold_H.unfold(3, patch_W, step_int[1])   
    if((img.size(3) - patch_W) % step_int[1] != 0) and keep_last_patch:
        patches_fold_HW = torch.cat((patches_fold_HW,patches_fold_H[:,:,:,-patch_W:,:].permute(0,1,2,4,3).unsqueeze(3)),dim=3)
    patches = patches_fold_HW.permute(2,3,0,1,4,5)
    patches = patches.reshape(-1,img.size(0),img.size(1),patch_H,patch_W)
    if(batch_first):
        patches = patches.permute(1,0,2,3,4)
    return patches.reshape(-1,3,patch_H,patch_W)

def prepare_image(image, resize = False, repeatNum = 1):
    if resize and min(image.size)>256:
        image = transforms.functional.resize(image,256)
    image = transforms.ToTensor()(image)
    return image.unsqueeze(0).repeat(repeatNum,1,1,1)

def print_network(net):
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print(net)
    print('Total number of parameters: %d' % num_params)

from typing import Optional, Union, Tuple, List, Iterable
import torch


def _adjust_dimensions(input_tensors: Union[torch.Tensor, Iterable[torch.Tensor]]):
    r"""Expands input tensors dimensions to 4D (N, C, H, W).
    """
    if isinstance(input_tensors, torch.Tensor):
        input_tensors = (input_tensors,)

    resized_tensors = []
    for tensor in input_tensors:
        tmp = tensor.clone()
        if tmp.dim() == 2:
            tmp = tmp.unsqueeze(0)
        if tmp.dim() == 3:
            tmp = tmp.unsqueeze(0)
        if tmp.dim() != 4 and tmp.dim() != 5:
            raise ValueError(f'Expected 2, 3, 4 or 5 dimensions (got {tensor.dim()})')
        resized_tensors.append(tmp)

    if len(resized_tensors) == 1:
        return resized_tensors[0]

    return tuple(resized_tensors)


def _validate_input(
        input_tensors: Union[torch.Tensor, Iterable[torch.Tensor]],
        allow_5d: bool,
        allow_negative: bool = False,
        kernel_size: Optional[int] = None,
        scale_weights: Union[Optional[Tuple[float]], Optional[List[float]], Optional[torch.Tensor]] = None,
        data_range: Optional[Union[float, int]] = None) -> None:

    if isinstance(input_tensors, torch.Tensor):
        input_tensors = (input_tensors,)

    assert isinstance(input_tensors, tuple)
    assert 0 < len(input_tensors) < 3, f'Expected one or two input tensors, got {len(input_tensors)}'

    min_n_dim = 2
    max_n_dim = 5 if allow_5d else 4
    for tensor in input_tensors:
        assert isinstance(tensor, torch.Tensor), f'Expected input to be torch.Tensor, got {type(tensor)}.'
        assert min_n_dim <= tensor.dim() <= max_n_dim, \
            f'Input images must be {min_n_dim}D - {max_n_dim}D tensors, got images of shape {tensor.size()}.'
        if not allow_negative:
            assert torch.all(tensor >= 0), 'All tensor values should be greater or equal than 0'
        if tensor.dim() == 5:
            assert tensor.size(-1) == 2, f'Expected Complex 5D tensor with (N, C, H, W, 2) size, got {tensor.size()}'
        if data_range is not None:
            assert data_range >= tensor.max(), \
                f'Data range should be greater or equal to maximum tensor value, got {data_range} and {tensor.max()}.'

    if len(input_tensors) == 2:
        assert input_tensors[0].size() == input_tensors[1].size(), \
            f'Input images must have the same dimensions, got {input_tensors[0].size()} and {input_tensors[1].size()}.'

    if kernel_size is not None:
        assert kernel_size % 2 == 1, f'Kernel size must be odd, got {kernel_size}.'
    if scale_weights is not None:
        assert isinstance(scale_weights, (list, tuple, torch.Tensor)), \
            f'Scale weights must be of type list, tuple or torch.Tensor, got {type(scale_weights)}.'
        if isinstance(scale_weights, (list, tuple)):
            scale_weights = torch.tensor(scale_weights)
        assert (scale_weights.dim() == 1), \
            f'Scale weights must be one dimensional, got {scale_weights.dim()}.'


def _validate_features(x: torch.Tensor, y: torch.Tensor) -> None:
    r"""Check, that computed features satisfy metric requirements.
    Args:
        x : Low-dimensional representation of predicted images.
        y : Low-dimensional representation of target images.
    """
    assert torch.is_tensor(x) and torch.is_tensor(y), \
        f"Both features should be torch.Tensors, got {type(x)} and {type(y)}"
    assert x.dim() == 2, \
        f"Predicted features must have shape (N_samples, encoder_dim), got {x.shape}"
    assert y.dim() == 2, \
        f"Target features must have shape  (N_samples, encoder_dim), got {y.shape}"
    assert x.size(1) == y.size(1), \
        f"Features dimensionalities should match, otherwise it won't be possible to correctly compute statistics. \
            Got {x.size(1)} and {y.size(1)}"
    assert x.device == y.device, "Both tensors should be on the same device"

import numpy as np
import os
import sys
import torch
from torchvision import models,transforms
import torch.nn as nn
import torch.nn.functional as F
import inspect
from numpy.fft import fft2, ifft2, fftshift, ifftshift
import math
#from utils import abs, real, imag, downsample, batch_fftshift2d, batch_ifftshift2d

MAX = nn.MaxPool2d((2,2), stride=1, padding=1)

def extract_patches_2d(img, patch_shape=[64, 64], step=[27,27], batch_first=False, keep_last_patch=False):
    patch_H, patch_W = patch_shape[0], patch_shape[1]
    if(img.size(2)<patch_H):
        num_padded_H_Top = (patch_H - img.size(2))//2
        num_padded_H_Bottom = patch_H - img.size(2) - num_padded_H_Top
        padding_H = nn.ConstantPad2d((0,0,num_padded_H_Top,num_padded_H_Bottom),0)
        img = padding_H(img)
    if(img.size(3)<patch_W):
        num_padded_W_Left = (patch_W - img.size(3))//2
        num_padded_W_Right = patch_W - img.size(3) - num_padded_W_Left
        padding_W = nn.ConstantPad2d((num_padded_W_Left,num_padded_W_Right,0,0),0)
        img = padding_W(img)
    step_int = [0,0]
    step_int[0] = int(patch_H*step[0]) if(isinstance(step[0], float)) else step[0]
    step_int[1] = int(patch_W*step[1]) if(isinstance(step[1], float)) else step[1]
    patches_fold_H = img.unfold(2, patch_H, step_int[0])
    if((img.size(2) - patch_H) % step_int[0] != 0) and keep_last_patch:
        patches_fold_H = torch.cat((patches_fold_H,img[:,:,-patch_H:,].permute(0,1,3,2).unsqueeze(2)),dim=2)
    patches_fold_HW = patches_fold_H.unfold(3, patch_W, step_int[1])   
    if((img.size(3) - patch_W) % step_int[1] != 0) and keep_last_patch:
        patches_fold_HW = torch.cat((patches_fold_HW,patches_fold_H[:,:,:,-patch_W:,:].permute(0,1,2,4,3).unsqueeze(3)),dim=3)
    patches = patches_fold_HW.permute(2,3,0,1,4,5)
    patches = patches.reshape(-1,img.size(0),img.size(1),patch_H,patch_W)
    if(batch_first):
        patches = patches.permute(1,0,2,3,4)
    return patches
    
def make_csf(rows, cols, nfreq=32):
    xvals = np.arange(-(cols - 1) / 2., (cols + 1) / 2.) 
    yvals = np.arange(-(rows - 1) / 2., (rows + 1) / 2.) 

    xplane,yplane=np.meshgrid(xvals, yvals)	# generate mesh
    plane=((xplane+1j *yplane)/cols)*2*nfreq
    radfreq=np.abs(plane)				# radial frequency

    w=0.7
    s=(1-w)/2*np.cos(4*np.angle(plane))+(1+w)/2
    radfreq=radfreq/s

    # Now generate the CSF
    csf = 2.6*(0.0192+0.114*radfreq)*np.exp(-(0.114*radfreq)**1.1)
    csf[radfreq < 7.8909]=0.9809

    return np.transpose(csf)

def get_moments(d,sk=False):
    # Return the first 4 moments of the data provided
    mean = torch.mean(d,dim=[3,4],keepdim=True)
    diffs = d - mean
    var = torch.mean(torch.pow(diffs, 2.0),dim=[3,4],keepdim=True)
    std = torch.pow(var+1e-12, 0.5)
    if sk:
        zscores = diffs / std
        skews = torch.mean(torch.pow(zscores, 3.0),dim=[3,4],keepdim=True)
        kurtoses = torch.mean(torch.pow(zscores, 4.0),dim=[3,4],keepdim=True) - 3.0  # excess kurtosis, should be 0 for Gaussian
        return mean,std,skews,kurtoses
    else:
        return mean,std

def ical_stat(x, p=16,s=4):
    B, C, H, W = x.shape
    x1 = extract_patches_2d(x,patch_shape=[p,p],step=[s,s])
    _,std,skews,kurt = get_moments(x1,sk=True)
    STD = std.reshape(B, C, (H-(p-s))//s, (W-(p-s))//s)
    SKEWS = skews.reshape(B, C, (H-(p-s))//s, (W-(p-s))//s)
    KURT = kurt.reshape(B, C, (H-(p-s))//s, (W-(p-s))//s)
    return STD, SKEWS, KURT # different with original version

def ical_std(x, p=16,s=4):
    B, C, H, W = x.shape
    x1 = extract_patches_2d(x,patch_shape=[p,p],step=[s,s])
    mean,std = get_moments(x1)
    mean = mean.reshape(B, C, (H-(p-s))//s, (W-(p-s))//s)
    std = std.reshape(B, C, (H-(p-s))//s, (W-(p-s))//s)

    return mean, std

def min_std(x, p=8,s=4):
    B, C, H, W = x.shape
    STD = torch.zeros_like(x)
    for i in range(0,H-p+1,s):
        for j in range(0,W-p+1,s):
            x1 = x[:,:,i:i+p,j:j+p]
            STD[:,:,i:i+s,j:j+s]=torch.min(torch.min(x1,2,keepdim=True)[0],3,keepdim=True)[0].repeat(1,1,s,s)
    return STD

def hi_index(ref_img, dst_img):
    k = 0.02874
    G = 0.5        
    C_slope = 1    
    Ci_thrsh= -5   
    Cd_thrsh= -5   
    # ms_scale= 1    

    ref = k * (ref_img+1e-12) ** (2.2/3)
    dst = k * (torch.abs(dst_img)+1e-12) ** (2.2/3)
    
    B, C, H, W = ref.shape

    csf = make_csf(H, W, 32)
    csf = torch.from_numpy(csf.reshape(1,1,H,W,1)).float().repeat(1,C,1,1,2).to(ref.device)
    x = torch.rfft(ref, 2, onesided=False)
    x1 = batch_fftshift2d(x)
    x2 = batch_ifftshift2d( x1 * csf )
    ref = real(torch.ifft(x2,2))
    x = torch.rfft(dst, 2, onesided=False)
    x1 = batch_fftshift2d(x)
    x2 = batch_ifftshift2d( x1 * csf )
    dst = real(torch.ifft(x2,2))

    m1_1,std_1 = ical_std(ref)
    B, C, H1, W1 = m1_1.shape
    # _,std_1 = ical_std(ref,p=8)
    # std_11 = min_std(std_1)
    std_1 = (-MAX(-std_1)/2)[:,:,:H1,:W1]
    _,std_2 = ical_std(dst-ref)

    BSIZE = 16
    eps = 1e-12
    Ci_ref = torch.log(torch.abs((std_1+eps)/(m1_1+eps)) )
    Ci_dst = torch.log(torch.abs((std_2+eps)/(m1_1+eps)) )
    Ci_dst = Ci_dst.masked_fill(m1_1 < G,-1000)
    idx1      = (Ci_ref > Ci_thrsh) & (Ci_dst > (C_slope * (Ci_ref - Ci_thrsh) + Cd_thrsh) ) 
    idx2      = (Ci_ref <= Ci_thrsh) & (Ci_dst > Cd_thrsh) 

    msk = Ci_ref.clone()
    msk = msk.masked_fill(~idx1,0)
    msk = msk.masked_fill(~idx2,0)
    msk[idx1] = Ci_dst[idx1] - (C_slope * (Ci_ref[idx1]-Ci_thrsh) + Cd_thrsh)
    msk[idx2] = Ci_dst[idx2] - Cd_thrsh

    win = torch.ones( (1,1,BSIZE, BSIZE) ).repeat(C,1,1,1).to(ref.device) / BSIZE**2
    xx = (ref_img-dst_img)**2
    # p = (BSIZE-1)//2
    # xx = F.pad(xx,(p,p,p,p),'reflect')
    lmse  = F.conv2d(xx, win, stride=4, padding =0, groups = C) 

    mp = msk * lmse
    # mp2 = mp[:,:, BSIZE+1:-BSIZE-1, BSIZE+1:-BSIZE-1]
    B, C, H, W = mp.shape
    return torch.norm( mp.reshape(B,C,-1) , dim=2 ) / math.sqrt( H*W ) * 200

def gaborconvolve(im):

    nscale          = 5      #Number of wavelet scales.
    norient         = 4      #Number of filter orientations.
    minWaveLength   = 3      #Wavelength of smallest scale filter.
    mult            = 3      #Scaling factor between successive filters.
    sigmaOnf        = 0.55   #Ratio of the standard deviation of the
    wavelength      = [minWaveLength,minWaveLength*mult,minWaveLength*mult**2, minWaveLength*mult**3, minWaveLength*mult**4]
    dThetaOnSigma   = 1.5    #Ratio of angular interval between filter orientations

    B, C, rows, cols = im.shape
    imagefft    = torch.rfft(im,2, onesided=False)            # Fourier transform of image

    # Pre-compute to speed up filter construction
    x = np.ones((rows,1)) * np.arange(-cols/2.,(cols/2.))/(cols/2.)
    y = np.dot(np.expand_dims(np.arange(-rows/2.,(rows/2.)),1) , np.ones((1,cols))/(rows/2.))
    radius = np.sqrt(x**2 + y**2)       # Matrix values contain *normalised* radius from centre.
    radius[int(np.round(rows/2+1)),int(np.round(cols/2+1))] = 1 # Get rid of the 0 radius value in the middle
    radius = np.log(radius+1e-12)

    theta = np.arctan2(-y,x)              # Matrix values contain polar angle.
    # (note -ve y is used to give +ve
    # anti-clockwise angles)
    sintheta = np.sin(theta)
    costheta = np.cos(theta)

    thetaSigma = math.pi/norient/dThetaOnSigma  # Calculate the standard deviation of the

    logGabors = []
    for s in range(nscale):                  # For each scale.
        # Construct the filter - first calculate the radial filter component.
        fo = 1.0/wavelength[s]                  # Centre frequency of filter.
        rfo = fo/0.5                         # Normalised radius from centre of frequency plane
        # corresponding to fo.
        tmp = -(2 * np.log(sigmaOnf)**2)
        tmp2= np.log(rfo)
        logGabors.append(np.exp( (radius-tmp2)**2 /tmp))
        logGabors[s][int(np.round(rows/2)), int(np.round(cols/2))]=0


    E0 = [[],[],[],[]]
    for o in range(norient):                   # For each orientation.
        angl = o*math.pi/norient           # Calculate filter angle.
        
        ds = sintheta * np.cos(angl) - costheta * np.sin(angl)     # Difference in sine.
        dc = costheta * np.cos(angl) + sintheta * np.sin(angl)     # Difference in cosine.
        dtheta = np.abs(np.arctan2(ds,dc))                           # Absolute angular distance.
        spread = np.exp((-dtheta**2) / (2 * thetaSigma**2))      # Calculate the angular filter component.
        
        for s in range(nscale):        # For each scale.

            filter = fftshift(logGabors[s] * spread)
            filter = torch.from_numpy(filter).reshape(1,1,rows,cols,1).repeat(1,C,1,1,2).to(im.device)
            # c =  imagefft * filter
            e0 = torch.ifft( imagefft * filter, 2 )
            E0[o].append(e0)

    return E0
        
def lo_index(ref, dst):
    gabRef  = gaborconvolve( ref )
    gabDst  = gaborconvolve( dst )
    s = [0.5/13.25, 0.75/13.25, 1/13.25, 5/13.25, 6/13.25]

    BSIZE = 16
    mp = 0
    for gb_i in range(4):
        for gb_j in range(5):
            stdref, skwref, krtref = ical_stat( abs( gabRef[gb_i][gb_j] ) )
            stddst, skwdst, krtdst  = ical_stat( abs( gabDst[gb_i][gb_j] ) )
            mp = mp + s[gb_i] * ( torch.abs( stdref - stddst ) + 2*torch.abs( skwref - skwdst ) +  torch.abs( krtref - krtdst ) ) 

    # mp2 = mp[:,:, BSIZE+1:-BSIZE-1, BSIZE+1:-BSIZE-1]
    B, C, rows, cols = mp.shape
    return torch.norm( mp.reshape(B,C,-1) , dim=2 ) / np.sqrt(rows * cols) 


def mad(x:torch.Tensor, y: torch.Tensor, reduction: str = 'mean',
            data_range: Union[int, float] = 1., scales: int = 3, subsample: bool = True,
            c: float = 30.0, alpha: float = 4.2) -> torch.Tensor:
    _validate_input(input_tensors=(x, y), allow_5d=False, scale_weights=None, data_range=data_range)
    x, y = _adjust_dimensions(input_tensors=(x, y))
    HI = hi_index(x, y)
    LO = lo_index(x, y)  
    thresh1   = 2.55
    thresh2   = 3.35
    b1        = math.exp(-thresh1/thresh2)
    b2        = 1 / (math.log(10)*thresh2)
    sig       = 1 / ( 1 + b1*HI**b2 ) 
    MAD = LO**(1-sig) * HI**(sig)
    if reduction == 'none':
        return MAD

    return {'mean': MAD.mean,
            'sum': MAD.sum
            }[reduction](dim=0)

class MAD(torch.nn.Module):
    # Refer to http://vision.eng.shizuoka.ac.jp/mod/page/view.php?id=23

    def __init__(self, reduction: Optional[str] = 'mean', data_range: Union[int, float] = 1.,
                 scales: int = 3, subsample: bool = True, c: float = 30.0, alpha: float = 4.2) -> None:
        super().__init__()
        self.reduction = reduction
        self.data_range = data_range
        self.mad = functools.partial(
            mad, scales=scales, subsample=subsample, c=c, alpha=alpha,
            data_range=data_range, reduction=reduction)

    def forward(self, prediction, target):
        return 1. - self.mad(x=prediction, y=target)


if __name__ == '__main__':
    from PIL import Image
    #import argparse
    #from utils import prepare_image

    # parser = argparse.ArgumentParser()
    # parser.add_argument('--ref', type=str, default='/content/r0.png')
    # parser.add_argument('--dist', type=str, default='/content/r1.png')
    # args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    ref = prepare_image(Image.open('/content/r0.png').convert("L"), repeatNum = 1).to(device)
    dist = prepare_image(Image.open('/content/r1.png').convert("L"), repeatNum = 1).to(device)

    model = MAD().to(device)
    score = model(dist, ref)
    print('score: %.4f', score.item())